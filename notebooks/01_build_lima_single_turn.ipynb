{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef3cb755",
   "metadata": {},
   "source": [
    "## Generate Instruction with Backward Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f02f20cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jiaxu/Etudes/llm/backtranslation/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['messages'],\n",
       "        num_rows: 1030\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['messages'],\n",
       "        num_rows: 300\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "DATASET = \"essobi/lima\"\n",
    "\n",
    "lima_dataset = load_dataset(DATASET)\n",
    "lima_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18b5fbc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['messages'],\n",
       "    num_rows: 1030\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lima_dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b006741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function datasets.load.load_dataset(path: str, name: Optional[str] = None, data_dir: Optional[str] = None, data_files: Union[str, collections.abc.Sequence[str], collections.abc.Mapping[str, Union[str, collections.abc.Sequence[str]]], NoneType] = None, split: Union[str, datasets.splits.Split, list[str], list[datasets.splits.Split], NoneType] = None, cache_dir: Optional[str] = None, features: Optional[datasets.features.features.Features] = None, download_config: Optional[datasets.download.download_config.DownloadConfig] = None, download_mode: Union[datasets.download.download_manager.DownloadMode, str, NoneType] = None, verification_mode: Union[datasets.utils.info_utils.VerificationMode, str, NoneType] = None, keep_in_memory: Optional[bool] = None, save_infos: bool = False, revision: Union[datasets.utils.version.Version, str, NoneType] = None, token: Union[bool, str, NoneType] = None, streaming: bool = False, num_proc: Optional[int] = None, storage_options: Optional[dict] = None, **config_kwargs) -> Union[datasets.dataset_dict.DatasetDict, datasets.arrow_dataset.Dataset, datasets.dataset_dict.IterableDatasetDict, datasets.iterable_dataset.IterableDataset]>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "# The path your script saved to\n",
    "local_path = \"../models/curated_lima\"\n",
    "\n",
    "# Load the dataset back into a new variable\n",
    "loaded_dataset = load_from_disk(local_path)\n",
    "\n",
    "load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "745452f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully loaded dataset with 10 rows.\n",
      "\n",
      "--- Generated Instructions (First 5 Samples) ---\n",
      "\n",
      "--- Sample #1 ---\n",
      "What is the response to the question, \"Do brain cells migrate?\"\n",
      "The question is relatively broad and one should take into account that the brain\n",
      "\n",
      "--- Sample #2 ---\n",
      "what is the best cpu architecture for a program to execute fastest?\n",
      "There is a general historical trend.\n",
      "In the olden days, memories were small, and\n",
      "\n",
      "--- Sample #3 ---\n",
      "\n",
      "Sure, please take a look at csvkit. It provides a set of tools that adhere to th\n",
      "\n",
      "--- Sample #4 ---\n",
      "What is the decay rate of a molecule with an ionization potential of 5eV?\n",
      "I'll answer this question from the theoretical side. The exponential behavior fo\n",
      "\n",
      "--- Sample #5 ---\n",
      ">  You can see all the commands in the ```git``` tutorial\n",
      "Remember that in ```git``` you have:\n",
      "\n",
      "* the ```HEAD``` pointer, which tells you \n",
      "\n",
      "--- Sample #6 ---\n",
      "What is the purpose of the com.google.common.hash API?\n",
      "The com.google.common.hash API offers:\n",
      "\n",
      "* A unified user-friendly API for all ha\n",
      "\n",
      "--- Sample #7 ---\n",
      "What is your response to this statement?\n",
      "It's not clear to me that Democrats are opposed to border-wall construction (you\n",
      "\n",
      "--- Sample #8 ---\n",
      "How can I convert a video to a gif that has no intermediate files?\n",
      "If you would prefer to avoid intermediate image files, the commands provided by \n",
      "\n",
      "--- Sample #9 ---\n",
      "### Explanation:\n",
      "Tor clients do not, in general, directly do DNS requests.  When you open a conne\n",
      "\n",
      "--- Sample #10 ---\n",
      "What is the type of the following expression?\n",
      "The difference is that the explicit type of the ```returnsNull()``` method affec\n"
     ]
    }
   ],
   "source": [
    "print(f\"✅ Successfully loaded dataset with {len(loaded_dataset)} rows.\")\n",
    "\n",
    "print(\"\\n--- Generated Instructions (First 5 Samples) ---\")\n",
    "# Iterate through the rows to display the newly generated instructions\n",
    "for i in range(len(loaded_dataset)):\n",
    "    instruction = loaded_dataset[i]['generated_instruction']\n",
    "    response = loaded_dataset[i]['output']  # Optionally show the response too\n",
    "\n",
    "    print(f\"\\n--- Sample #{i+1} ---\")\n",
    "    print(f\"{instruction}\")\n",
    "    # Print first 80 chars of response\n",
    "    print(f\"{response[:80]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ac631ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['messages'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter out multi-turn and sample 150\n",
    "\n",
    "def is_single_turn(example):\n",
    "    return len(example['messages']) == 2\n",
    "\n",
    "\n",
    "single_turn_dataset = lima_dataset['train'].filter(is_single_turn)\n",
    "single_turn_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0098c893",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [{'content': 'Can brain cells move? By movement I mean long distance migration (preferably within the brain only).',\n",
       "   'role': 'user'},\n",
       "  {'content': 'The question is relatively broad and one should take into account that the brain not only consists of neurons, but also glial cells (supportive cells) and pre-mitotic neuronal stem cells. Furthermore, as critical fellow-scientists have indicated, developmental stage is very important, as the developing embryonic brain is very different from the adult brain.\\nHowever, after sifting through various publications, the answer to the question is actually remarkably simple: Yes, brain cells migrate.\\nIn  the adult brain glial cells migrate in the brain (Klämbt, 2009). Glial cells are involved in a myriad of functions, but a notable example of migrating glial cells are the oligodendrocytes that migrate relative long distances to find their target axons onto which they wrap themselves to form the insulating myelin sheath (Tsai and Miller, 2002).\\nNeuronal stem cells migrate over long distances in response to injury (Imitola et al., 2004) and they migrate from specific stem-cell locations (e.g., hippocampus and subventricular zone) to other regions (Clarke, 2003).\\nPost-mitotic, but non-differentiated neurons have been shown to migrate in the adult brain in fish (Scott et al., 2012), and in mammals and non-human primates as well (Sawada et al., 2011).\\nNot surprisingly, glial cells, stem cells and neurons also migrate during embryonic development. Most notably, post-mitotic neurons destined to fulfill peripheral functions have to migrate over relatively long distances from the neural crest to their target locations (Neuroscience, 2nd ed, Neuronal Migration).',\n",
       "   'role': 'assistant'}]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_turn_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c254ddc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Can brain cells move? By movement I mean long distance migration (preferably within the brain only).',\n",
       " 'output': 'The question is relatively broad and one should take into account that the brain not only consists of neurons, but also glial cells (supportive cells) and pre-mitotic neuronal stem cells. Furthermore, as critical fellow-scientists have indicated, developmental stage is very important, as the developing embryonic brain is very different from the adult brain.\\nHowever, after sifting through various publications, the answer to the question is actually remarkably simple: Yes, brain cells migrate.\\nIn  the adult brain glial cells migrate in the brain (Klämbt, 2009). Glial cells are involved in a myriad of functions, but a notable example of migrating glial cells are the oligodendrocytes that migrate relative long distances to find their target axons onto which they wrap themselves to form the insulating myelin sheath (Tsai and Miller, 2002).\\nNeuronal stem cells migrate over long distances in response to injury (Imitola et al., 2004) and they migrate from specific stem-cell locations (e.g., hippocampus and subventricular zone) to other regions (Clarke, 2003).\\nPost-mitotic, but non-differentiated neurons have been shown to migrate in the adult brain in fish (Scott et al., 2012), and in mammals and non-human primates as well (Sawada et al., 2011).\\nNot surprisingly, glial cells, stem cells and neurons also migrate during embryonic development. Most notably, post-mitotic neurons destined to fulfill peripheral functions have to migrate over relatively long distances from the neural crest to their target locations (Neuroscience, 2nd ed, Neuronal Migration).'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build dataset\n",
    "def build_dataset(raw):\n",
    "    processed = []\n",
    "\n",
    "    for row in raw:\n",
    "        instruction = row['messages'][0]['content']\n",
    "        output = row['messages'][1]['content']\n",
    "        processed.append({'instruction': instruction, 'output': output})\n",
    "\n",
    "    return processed\n",
    "\n",
    "processed_dataset = build_dataset(single_turn_dataset)\n",
    "processed_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f35e91d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 1000/1000 [00:00<00:00, 291980.79 examples/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 74.02ba/s]\n",
      "Processing Files (1 / 1): 100%|██████████| 1.62MB / 1.62MB,  0.00B/s  \n",
      "New Data Upload: |          |  0.00B /  0.00B,  0.00B/s  \n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.21s/ shards]\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/xujia118/lima-single-turn/commit/279266fc89b3ab5a4db10cfbcb53d8d9fafca297', commit_message='Upload dataset', commit_description='', oid='279266fc89b3ab5a4db10cfbcb53d8d9fafca297', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/xujia118/lima-single-turn', endpoint='https://huggingface.co', repo_type='dataset', repo_id='xujia118/lima-single-turn'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "processed_hf = Dataset.from_list(processed_dataset)\n",
    "processed_hf.save_to_disk(\"lima-single-turn\")\n",
    "processed_hf.push_to_hub(\"xujia118/lima-single-turn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be22c982",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "BASE_MODEL_ID = \"meta-llama/Llama-2-7b-hf\"\n",
    "NEW_MODEL_ID = \"xujia118/backwards-llama2-7b-guanaco\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c7a3f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jiaxu/Etudes/llm/backtranslation/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.14s/it]\n",
      "/Users/jiaxu/Etudes/llm/backtranslation/venv/lib/python3.12/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from peft import PeftModel\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_ID,\n",
    "    device_map=\"auto\",\n",
    "    dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID)\n",
    "\n",
    "key_mapping = {\n",
    "    'base_model.model.model.model.model': '',  # Strips the erroneous prefix\n",
    "}\n",
    "# Load LoRA on top\n",
    "model = PeftModel.from_pretrained(model, NEW_MODEL_ID, key_mapping=key_mapping)\n",
    "model.eval()\n",
    "\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=10,\n",
    "    temperature=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44689aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt(response_text):\n",
    "    return f\"\"\"\n",
    "    Below is a response that answers to a question. \n",
    "    Your task is to write a question that would most appropriately produce the response.\n",
    "\n",
    "    ### Response:\n",
    "    {response_text}\n",
    "\n",
    "    ### Question:\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe48a9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def process_generated_text(prompt: str, full_text: str) -> str:\n",
    "    generated = full_text[len(prompt):].strip()\n",
    "    generated = generated.split(\"\\n\")[0].strip()\n",
    "    generated = re.sub(r'^\\s*(\\d+[\\.\\)]|\\-|\\*)\\s*', '', generated)\n",
    "    return generated.strip()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "78e7a3f3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'processed_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     22\u001b[39m         all_instructions.extend(processed)\n\u001b[32m     24\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m all_instructions\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m prompt_data = build_dataset(\u001b[43mprocessed_dataset\u001b[49m)\n\u001b[32m     29\u001b[39m SAVE_DIR = \u001b[33m\"\u001b[39m\u001b[33mprocessed-lima-backward-augmented-new.json\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(SAVE_DIR, \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m, encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[31mNameError\u001b[39m: name 'processed_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def build_dataset(data):\n",
    "    batch_size = 4\n",
    "    all_instructions = []\n",
    "\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        batch = data[i:i+batch_size]\n",
    "        batch_outputs = [format_prompt(item[\"output\"]) for item in batch]\n",
    "\n",
    "        print(f\"Batch number {i + 1} is ready, starting to generate.\")\n",
    "        \n",
    "        # Generate instructions for the whole batch\n",
    "        batch_generated = generator(batch_outputs)\n",
    "        \n",
    "        # Clean each generated instruction\n",
    "        processed = [\n",
    "            process_generated_text(prompt, gen[0][\"generated_text\"])\n",
    "            for prompt, gen in zip(batch_outputs, batch_generated)\n",
    "        ]\n",
    "        \n",
    "        all_instructions.extend(processed)\n",
    "    \n",
    "    return all_instructions\n",
    "\n",
    "\n",
    "prompt_data = build_dataset(processed_dataset[:200])\n",
    "\n",
    "SAVE_DIR = \"processed-lima-backward-augmented-new.json\"\n",
    "\n",
    "with open(SAVE_DIR, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(prompt_data, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a418b7",
   "metadata": {},
   "source": [
    "## Self-Curation\n",
    "- Goal: filter out bad examples and only keep high quality examples\n",
    "- Might as well use Gemini Api, it's faster and free for 250 calls\n",
    "- Need another alpaca prompt\n",
    "- Push the dataset to HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "69df61ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine generated instruction, output_text, original_instruction\n",
    "import json\n",
    "\n",
    "\n",
    "def load_json(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def combine_original_generated():\n",
    "    combined = []\n",
    "\n",
    "    original_lima = load_json(\"processed_dataset.json\")\n",
    "    generated_instructions = load_json(\"processed-lima-backward-augmented.json\")\n",
    "\n",
    "    for orig, gen in zip(original_lima, generated_instructions):\n",
    "        combined.append({\n",
    "            \"generated_instruction\": gen,\n",
    "            \"output_text\": orig[\"output\"],\n",
    "            \"original_instruction\": orig[\"instruction\"]\n",
    "        })\n",
    "    \n",
    "    return combined\n",
    "\n",
    "combined = combine_original_generated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d71fb15d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'generated_instruction': 'Do brain cells migrate in the adult brain?',\n",
       " 'output_text': 'The question is relatively broad and one should take into account that the brain not only consists of neurons, but also glial cells (supportive cells) and pre-mitotic neuronal stem cells. Furthermore, as critical fellow-scientists have indicated, developmental stage is very important, as the developing embryonic brain is very different from the adult brain.\\nHowever, after sifting through various publications, the answer to the question is actually remarkably simple: Yes, brain cells migrate.\\nIn  the adult brain glial cells migrate in the brain (Klämbt, 2009). Glial cells are involved in a myriad of functions, but a notable example of migrating glial cells are the oligodendrocytes that migrate relative long distances to find their target axons onto which they wrap themselves to form the insulating myelin sheath (Tsai and Miller, 2002).\\nNeuronal stem cells migrate over long distances in response to injury (Imitola et al., 2004) and they migrate from specific stem-cell locations (e.g., hippocampus and subventricular zone) to other regions (Clarke, 2003).\\nPost-mitotic, but non-differentiated neurons have been shown to migrate in the adult brain in fish (Scott et al., 2012), and in mammals and non-human primates as well (Sawada et al., 2011).\\nNot surprisingly, glial cells, stem cells and neurons also migrate during embryonic development. Most notably, post-mitotic neurons destined to fulfill peripheral functions have to migrate over relatively long distances from the neural crest to their target locations (Neuroscience, 2nd ed, Neuronal Migration).',\n",
       " 'original_instruction': 'Can brain cells move? By movement I mean long distance migration (preferably within the brain only).'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8e55c76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"combined.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(combined, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "334f9d61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03d791eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed the model with the data and a new alpaca prompt\n",
    "\n",
    "def format_self_curation_prompt(row):\n",
    "    return f\"\"\"\n",
    "Rate how well the generated instruction matches the original instruction.\n",
    "Return ONLY a single integer from 1 to 5.   \n",
    "\n",
    "Generated instruction:\n",
    "{row[\"generated_instruction\"]}\n",
    "\n",
    "Original instruction:\n",
    "{row[\"original_instruction\"]}\n",
    "\n",
    "Score: \n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "742be915",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "GEMINI_URL = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent\"\n",
    "\n",
    "\n",
    "def generate_with_gemini(prompt):\n",
    "    headers = {\n",
    "        \"X-Goog-Api-Key\": GOOGLE_API_KEY,\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    data = {\n",
    "        \"contents\": [{\n",
    "            \"parts\": [{\n",
    "                \"text\": prompt\n",
    "            }]\n",
    "        }],\n",
    "        \"generationConfig\": {\n",
    "            \"temperature\": 0.8\n",
    "        }\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        resp = requests.post(GEMINI_URL, headers=headers, json=data)\n",
    "        resp.raise_for_status()\n",
    "        answer_text = resp.json(\n",
    "        )['candidates'][0]['content']['parts'][0]['text']\n",
    "        return answer_text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return \"\"\n",
    "    finally:\n",
    "        time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a53202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 0...\n",
      "Generating 10...\n",
      "Generating 20...\n",
      "Generating 30...\n",
      "Generating 40...\n",
      "Generating 50...\n",
      "Generating 60...\n",
      "Generating 70...\n",
      "Generating 80...\n",
      "Generating 90...\n",
      "Generating 100...\n",
      "Generating 110...\n",
      "Generating 120...\n",
      "Generating 130...\n",
      "Generating 140...\n",
      "Generating 150...\n",
      "Generating 160...\n",
      "Generating 170...\n",
      "Generating 180...\n",
      "Generating 190...\n",
      "Error: 429 Client Error: Too Many Requests for url: https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent\n",
      "Error: 429 Client Error: Too Many Requests for url: https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent\n",
      "Error: 429 Client Error: Too Many Requests for url: https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent\n",
      "Error: 429 Client Error: Too Many Requests for url: https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent\n",
      "Error: 429 Client Error: Too Many Requests for url: https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent\n",
      "Error: 429 Client Error: Too Many Requests for url: https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def build_curated_dataset(data):\n",
    "    for idx, row in enumerate(data):\n",
    "        if idx % 10 == 0:\n",
    "            print(f\"Generating {idx}...\")\n",
    "        prompt = format_self_curation_prompt(row)\n",
    "        gemini_answer = generate_with_gemini(prompt)\n",
    "        row[\"score\"] = gemini_answer\n",
    "    \n",
    "    return data\n",
    "\n",
    "    # all_ratings = []\n",
    "    # batch_size = 10\n",
    "\n",
    "    # for i in range(0, len(data), batch_size):\n",
    "    #     batch = data[i:i+batch_size]\n",
    "    #     batch_prompts = [format_self_curation_prompt(row) for row in batch]\n",
    "\n",
    "    #     print(f\"Batch number {i} is ready, starting to generate.\")\n",
    "    #     batch_generated = generator(batch_prompts)\n",
    "\n",
    "    #     all_ratings.extend(batch_generated)\n",
    "\n",
    "    # return all_ratings\n",
    "\n",
    "\n",
    "with open(\"combined.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    combined = json.load(f)\n",
    "    \n",
    "scored_dataset = build_curated_dataset(combined)\n",
    "\n",
    "with open(\"scored_dataset.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(scored_dataset, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5348acb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"scored_dataset.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    scored_dataset = json.load(f)\n",
    "\n",
    "best_samples = []\n",
    "for row in scored_dataset:\n",
    "    try:\n",
    "        if int(row[\"score\"]) >= 4:\n",
    "            best_samples.append(row)\n",
    "    except:\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf2b0e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "slim_ds = []\n",
    "for sample in best_samples:\n",
    "    slim_ds.append({\n",
    "        \"generated_instruction\": sample[\"generated_instruction\"],\n",
    "        \"output_text\": sample[\"output_text\"]\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "521b57bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 239.91ba/s]\n",
      "Processing Files (1 / 1): 100%|██████████| 92.5kB / 92.5kB,  154kB/s  \n",
      "New Data Upload: 100%|██████████| 92.5kB / 92.5kB,  154kB/s  \n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.33s/ shards]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/xujia118/new_curated_lima/commit/f16a75879b98628de15c2eca6102859c28ed90fc', commit_message='Upload dataset', commit_description='', oid='f16a75879b98628de15c2eca6102859c28ed90fc', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/xujia118/new_curated_lima', endpoint='https://huggingface.co', repo_type='dataset', repo_id='xujia118/new_curated_lima'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "dataset = Dataset.from_list(slim_ds)\n",
    "dataset.push_to_hub(\"xujia118/new_curated_lima\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90ec678",
   "metadata": {},
   "source": [
    "## Flow to tokenize data and push to HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e836cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "final_ds = DatasetDict({\n",
    "    \"train\": train_ds,\n",
    "    \"test\": test_ds\n",
    "})\n",
    "\n",
    "final_ds.save_to_disk(\"my_prepared_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227bff97",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This will create\n",
    "\n",
    "my_prepared_dataset/\n",
    "    dataset_info.json\n",
    "    train/\n",
    "        data-00000-of-00001.arrow\n",
    "        state.json\n",
    "    test/\n",
    "        data-00000-of-00001.arrow\n",
    "        state.json\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62809e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push to HF\n",
    "\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "api = HfApi()\n",
    "api.upload_folder(\n",
    "    folder_path=\"my_prepared_dataset\",\n",
    "    repo_id=\"my_username/my_prepared_dataset\",\n",
    "    repo_type=\"dataset\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a469557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load in GPU instance\n",
    "\n",
    "from datasets import load_from_disk, load_dataset\n",
    "\n",
    "ds = load_dataset(\"my_username/my_prepared_dataset\")\n",
    "\n",
    "# or copy from scp/S3\n",
    "ds = load_from_disk(\"my_prepared_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4031c027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.1\n",
      "CUDA available: False\n",
      "CUDA version PyTorch is using: None\n",
      "cuDNN version: None\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA version PyTorch is using:\", torch.version.cuda)\n",
    "print(\"cuDNN version:\", torch.backends.cudnn.version())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
