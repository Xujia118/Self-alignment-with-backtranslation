{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef3cb755",
   "metadata": {},
   "source": [
    "## Generate Instruction with Backward Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f02f20cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jiaxu/Etudes/llm/backtranslation/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['messages'],\n",
       "        num_rows: 1030\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['messages'],\n",
       "        num_rows: 300\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "DATASET = \"essobi/lima\"\n",
    "\n",
    "lima_dataset = load_dataset(DATASET)\n",
    "lima_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18b5fbc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [{'content': 'Can brain cells move? By movement I mean long distance migration (preferably within the brain only).',\n",
       "   'role': 'user'},\n",
       "  {'content': 'The question is relatively broad and one should take into account that the brain not only consists of neurons, but also glial cells (supportive cells) and pre-mitotic neuronal stem cells. Furthermore, as critical fellow-scientists have indicated, developmental stage is very important, as the developing embryonic brain is very different from the adult brain.\\nHowever, after sifting through various publications, the answer to the question is actually remarkably simple: Yes, brain cells migrate.\\nIn  the adult brain glial cells migrate in the brain (Klämbt, 2009). Glial cells are involved in a myriad of functions, but a notable example of migrating glial cells are the oligodendrocytes that migrate relative long distances to find their target axons onto which they wrap themselves to form the insulating myelin sheath (Tsai and Miller, 2002).\\nNeuronal stem cells migrate over long distances in response to injury (Imitola et al., 2004) and they migrate from specific stem-cell locations (e.g., hippocampus and subventricular zone) to other regions (Clarke, 2003).\\nPost-mitotic, but non-differentiated neurons have been shown to migrate in the adult brain in fish (Scott et al., 2012), and in mammals and non-human primates as well (Sawada et al., 2011).\\nNot surprisingly, glial cells, stem cells and neurons also migrate during embryonic development. Most notably, post-mitotic neurons destined to fulfill peripheral functions have to migrate over relatively long distances from the neural crest to their target locations (Neuroscience, 2nd ed, Neuronal Migration).',\n",
       "   'role': 'assistant'}]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lima_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ac631ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['messages'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter out multi-turn and sample 150\n",
    "\n",
    "def is_single_turn(example):\n",
    "    return len(example['messages']) == 2\n",
    "\n",
    "\n",
    "single_turn_dataset = lima_dataset['train'].filter(is_single_turn)\n",
    "single_turn_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0098c893",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [{'content': 'Can brain cells move? By movement I mean long distance migration (preferably within the brain only).',\n",
       "   'role': 'user'},\n",
       "  {'content': 'The question is relatively broad and one should take into account that the brain not only consists of neurons, but also glial cells (supportive cells) and pre-mitotic neuronal stem cells. Furthermore, as critical fellow-scientists have indicated, developmental stage is very important, as the developing embryonic brain is very different from the adult brain.\\nHowever, after sifting through various publications, the answer to the question is actually remarkably simple: Yes, brain cells migrate.\\nIn  the adult brain glial cells migrate in the brain (Klämbt, 2009). Glial cells are involved in a myriad of functions, but a notable example of migrating glial cells are the oligodendrocytes that migrate relative long distances to find their target axons onto which they wrap themselves to form the insulating myelin sheath (Tsai and Miller, 2002).\\nNeuronal stem cells migrate over long distances in response to injury (Imitola et al., 2004) and they migrate from specific stem-cell locations (e.g., hippocampus and subventricular zone) to other regions (Clarke, 2003).\\nPost-mitotic, but non-differentiated neurons have been shown to migrate in the adult brain in fish (Scott et al., 2012), and in mammals and non-human primates as well (Sawada et al., 2011).\\nNot surprisingly, glial cells, stem cells and neurons also migrate during embryonic development. Most notably, post-mitotic neurons destined to fulfill peripheral functions have to migrate over relatively long distances from the neural crest to their target locations (Neuroscience, 2nd ed, Neuronal Migration).',\n",
       "   'role': 'assistant'}]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_turn_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c254ddc2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'single_turn_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      8\u001b[39m         processed.append({\u001b[33m'\u001b[39m\u001b[33minstruction\u001b[39m\u001b[33m'\u001b[39m: instruction, \u001b[33m'\u001b[39m\u001b[33moutput\u001b[39m\u001b[33m'\u001b[39m: output})\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m processed\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m processed_dataset = build_dataset(\u001b[43msingle_turn_dataset\u001b[49m)\n\u001b[32m     13\u001b[39m processed_dataset[\u001b[32m0\u001b[39m]\n",
      "\u001b[31mNameError\u001b[39m: name 'single_turn_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# Build dataset\n",
    "def build_dataset(raw):\n",
    "    processed = []\n",
    "\n",
    "    for row in raw:\n",
    "        instruction = row['messages'][0]['content']\n",
    "        output = row['messages'][1]['content']\n",
    "        processed.append({'instruction': instruction, 'output': output})\n",
    "\n",
    "    return processed\n",
    "\n",
    "processed_dataset = build_dataset(single_turn_dataset)\n",
    "processed_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f35e91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "SAVE_DIR=\"processed_dataset.json\"\n",
    "\n",
    "with open(SAVE_DIR, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(processed_dataset, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be22c982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate instructions with backwards model\n",
    "\n",
    "'''\n",
    "backwards model is trained to predict instruction with output.\n",
    "Here the sampled dataset is in normal instruction: output order\n",
    "So we need to extract the output, at index 1.\n",
    "\n",
    "Then we use the backwards model to predict instructions.\n",
    "Some instructions will be of high quality, some will be of low quality.\n",
    "In the next step, we want to curate the high quality instructions.\n",
    "Then we train the backward model with curated dataset.\n",
    "'''\n",
    "\n",
    "BASE_MODEL_ID = \"meta-llama/Llama-2-7b-hf\"\n",
    "NEW_MODEL_ID = \"xujia118/backwards-llama2-7b-guanaco\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c7a3f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jiaxu/Etudes/llm/backtranslation/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.14s/it]\n",
      "/Users/jiaxu/Etudes/llm/backtranslation/venv/lib/python3.12/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from peft import PeftModel\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_ID,\n",
    "    device_map=\"auto\",\n",
    "    dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID)\n",
    "\n",
    "key_mapping = {\n",
    "    'base_model.model.model.model.model': '',  # Strips the erroneous prefix\n",
    "}\n",
    "# Load LoRA on top\n",
    "model = PeftModel.from_pretrained(model, NEW_MODEL_ID, key_mapping=key_mapping)\n",
    "model.eval()\n",
    "\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=10,\n",
    "    temperature=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44689aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt(response_text):\n",
    "    return f\"\"\"\n",
    "    Below is a response that answers to a question. \n",
    "    Your task is to write a question that would most appropriately produce the response.\n",
    "\n",
    "    ### Response:\n",
    "    {response_text}\n",
    "\n",
    "    ### Question:\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe48a9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def process_generated_text(prompt: str, full_text: str) -> str:\n",
    "    generated = full_text[len(prompt):].strip()\n",
    "    generated = generated.split(\"\\n\")[0].strip()\n",
    "    generated = re.sub(r'^\\s*(\\d+[\\.\\)]|\\-|\\*)\\s*', '', generated)\n",
    "    return generated.strip()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "78e7a3f3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'processed_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     22\u001b[39m         all_instructions.extend(processed)\n\u001b[32m     24\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m all_instructions\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m prompt_data = build_dataset(\u001b[43mprocessed_dataset\u001b[49m)\n\u001b[32m     29\u001b[39m SAVE_DIR = \u001b[33m\"\u001b[39m\u001b[33mprocessed-lima-backward-augmented-new.json\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(SAVE_DIR, \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m, encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[31mNameError\u001b[39m: name 'processed_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def build_dataset(data):\n",
    "    batch_size = 4\n",
    "    all_instructions = []\n",
    "\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        batch = data[i:i+batch_size]\n",
    "        batch_outputs = [format_prompt(item[\"output\"]) for item in batch]\n",
    "\n",
    "        print(f\"Batch number {i + 1} is ready, starting to generate.\")\n",
    "        \n",
    "        # Generate instructions for the whole batch\n",
    "        batch_generated = generator(batch_outputs)\n",
    "        \n",
    "        # Clean each generated instruction\n",
    "        processed = [\n",
    "            process_generated_text(prompt, gen[0][\"generated_text\"])\n",
    "            for prompt, gen in zip(batch_outputs, batch_generated)\n",
    "        ]\n",
    "        \n",
    "        all_instructions.extend(processed)\n",
    "    \n",
    "    return all_instructions\n",
    "\n",
    "\n",
    "prompt_data = build_dataset(processed_dataset[:200])\n",
    "\n",
    "SAVE_DIR = \"processed-lima-backward-augmented-new.json\"\n",
    "\n",
    "with open(SAVE_DIR, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(prompt_data, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a418b7",
   "metadata": {},
   "source": [
    "## Self-Curation\n",
    "- Goal: filter out bad examples and only keep high quality examples\n",
    "- Might as well use Gemini Api, it's faster and free for 250 calls\n",
    "- Need another alpaca prompt\n",
    "- Push the dataset to HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "69df61ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine generated instruction, output_text, original_instruction\n",
    "import json\n",
    "\n",
    "\n",
    "def load_json(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def combine_original_generated():\n",
    "    combined = []\n",
    "\n",
    "    original_lima = load_json(\"processed_dataset.json\")\n",
    "    generated_instructions = load_json(\"processed-lima-backward-augmented.json\")\n",
    "\n",
    "    for orig, gen in zip(original_lima, generated_instructions):\n",
    "        combined.append({\n",
    "            \"generated_instruction\": gen,\n",
    "            \"output_text\": orig[\"output\"],\n",
    "            \"original_instruction\": orig[\"instruction\"]\n",
    "        })\n",
    "    \n",
    "    return combined\n",
    "\n",
    "combined = combine_original_generated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d71fb15d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'generated_instruction': 'Do brain cells migrate in the adult brain?',\n",
       " 'output_text': 'The question is relatively broad and one should take into account that the brain not only consists of neurons, but also glial cells (supportive cells) and pre-mitotic neuronal stem cells. Furthermore, as critical fellow-scientists have indicated, developmental stage is very important, as the developing embryonic brain is very different from the adult brain.\\nHowever, after sifting through various publications, the answer to the question is actually remarkably simple: Yes, brain cells migrate.\\nIn  the adult brain glial cells migrate in the brain (Klämbt, 2009). Glial cells are involved in a myriad of functions, but a notable example of migrating glial cells are the oligodendrocytes that migrate relative long distances to find their target axons onto which they wrap themselves to form the insulating myelin sheath (Tsai and Miller, 2002).\\nNeuronal stem cells migrate over long distances in response to injury (Imitola et al., 2004) and they migrate from specific stem-cell locations (e.g., hippocampus and subventricular zone) to other regions (Clarke, 2003).\\nPost-mitotic, but non-differentiated neurons have been shown to migrate in the adult brain in fish (Scott et al., 2012), and in mammals and non-human primates as well (Sawada et al., 2011).\\nNot surprisingly, glial cells, stem cells and neurons also migrate during embryonic development. Most notably, post-mitotic neurons destined to fulfill peripheral functions have to migrate over relatively long distances from the neural crest to their target locations (Neuroscience, 2nd ed, Neuronal Migration).',\n",
       " 'original_instruction': 'Can brain cells move? By movement I mean long distance migration (preferably within the brain only).'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8e55c76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"combined.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(combined, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "334f9d61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03d791eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed the model with the data and a new alpaca prompt\n",
    "\n",
    "def format_self_curation_prompt(row):\n",
    "    return f\"\"\"\n",
    "Rate how well the generated instruction matches the original instruction.\n",
    "Return ONLY a single integer from 1 to 5.   \n",
    "\n",
    "Generated instruction:\n",
    "{row[\"generated_instruction\"]}\n",
    "\n",
    "Original instruction:\n",
    "{row[\"original_instruction\"]}\n",
    "\n",
    "Score: \n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "742be915",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "GEMINI_URL = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent\"\n",
    "\n",
    "\n",
    "def generate_with_gemini(prompt):\n",
    "    headers = {\n",
    "        \"X-Goog-Api-Key\": GOOGLE_API_KEY,\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    data = {\n",
    "        \"contents\": [{\n",
    "            \"parts\": [{\n",
    "                \"text\": prompt\n",
    "            }]\n",
    "        }],\n",
    "        \"generationConfig\": {\n",
    "            \"temperature\": 0.8\n",
    "        }\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        resp = requests.post(GEMINI_URL, headers=headers, json=data)\n",
    "        resp.raise_for_status()\n",
    "        answer_text = resp.json(\n",
    "        )['candidates'][0]['content']['parts'][0]['text']\n",
    "        return answer_text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return \"\"\n",
    "    finally:\n",
    "        time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a53202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 0...\n",
      "Generating 10...\n",
      "Generating 20...\n",
      "Generating 30...\n",
      "Generating 40...\n",
      "Generating 50...\n",
      "Generating 60...\n",
      "Generating 70...\n",
      "Generating 80...\n",
      "Generating 90...\n",
      "Generating 100...\n",
      "Generating 110...\n",
      "Generating 120...\n",
      "Generating 130...\n",
      "Generating 140...\n",
      "Generating 150...\n",
      "Generating 160...\n",
      "Generating 170...\n",
      "Generating 180...\n",
      "Generating 190...\n",
      "Error: 429 Client Error: Too Many Requests for url: https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent\n",
      "Error: 429 Client Error: Too Many Requests for url: https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent\n",
      "Error: 429 Client Error: Too Many Requests for url: https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent\n",
      "Error: 429 Client Error: Too Many Requests for url: https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent\n",
      "Error: 429 Client Error: Too Many Requests for url: https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent\n",
      "Error: 429 Client Error: Too Many Requests for url: https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def build_curated_dataset(data):\n",
    "    for idx, row in enumerate(data):\n",
    "        if idx % 10 == 0:\n",
    "            print(f\"Generating {idx}...\")\n",
    "        prompt = format_self_curation_prompt(row)\n",
    "        gemini_answer = generate_with_gemini(prompt)\n",
    "        row[\"score\"] = gemini_answer\n",
    "    \n",
    "    return data\n",
    "\n",
    "    # all_ratings = []\n",
    "    # batch_size = 10\n",
    "\n",
    "    # for i in range(0, len(data), batch_size):\n",
    "    #     batch = data[i:i+batch_size]\n",
    "    #     batch_prompts = [format_self_curation_prompt(row) for row in batch]\n",
    "\n",
    "    #     print(f\"Batch number {i} is ready, starting to generate.\")\n",
    "    #     batch_generated = generator(batch_prompts)\n",
    "\n",
    "    #     all_ratings.extend(batch_generated)\n",
    "\n",
    "    # return all_ratings\n",
    "\n",
    "\n",
    "with open(\"combined.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    combined = json.load(f)\n",
    "    \n",
    "scored_dataset = build_curated_dataset(combined)\n",
    "\n",
    "with open(\"scored_dataset.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(scored_dataset, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5348acb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"scored_dataset.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    scored_dataset = json.load(f)\n",
    "\n",
    "best_samples = []\n",
    "for row in scored_dataset:\n",
    "    try:\n",
    "        if int(row[\"score\"]) >= 4:\n",
    "            best_samples.append(row)\n",
    "    except:\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf2b0e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "slim_ds = []\n",
    "for sample in best_samples:\n",
    "    slim_ds.append({\n",
    "        \"generated_instruction\": sample[\"generated_instruction\"],\n",
    "        \"output_text\": sample[\"output_text\"]\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "521b57bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 239.91ba/s]\n",
      "Processing Files (1 / 1): 100%|██████████| 92.5kB / 92.5kB,  154kB/s  \n",
      "New Data Upload: 100%|██████████| 92.5kB / 92.5kB,  154kB/s  \n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.33s/ shards]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/xujia118/new_curated_lima/commit/f16a75879b98628de15c2eca6102859c28ed90fc', commit_message='Upload dataset', commit_description='', oid='f16a75879b98628de15c2eca6102859c28ed90fc', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/xujia118/new_curated_lima', endpoint='https://huggingface.co', repo_type='dataset', repo_id='xujia118/new_curated_lima'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "dataset = Dataset.from_list(slim_ds)\n",
    "dataset.push_to_hub(\"xujia118/new_curated_lima\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
